{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!apt-get install -y swig\n!pip install box2d-py gymnasium[box2d]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install stable-baselines3\n!pip install gymnasium\n!pip install imageio\n!pip install pygame","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport warnings\nimport gymnasium as gym\nimport torch\nimport torch.nn as nn\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor\nfrom stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback\nfrom stable_baselines3.common.utils import set_random_seed\nfrom gymnasium.wrappers import RecordVideo\nimport numpy as np\n\nos.environ[\"XDG_RUNTIME_DIR\"] = \"/tmp\"\nwarnings.filterwarnings(\"ignore\")\n\ndef ensure_dir(path):\n    Path(path).mkdir(parents=True, exist_ok=True)\n\nclass CustomRewardWrapper(gym.Wrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        \n    def step(self, action):\n        obs, reward, terminated, truncated, info = self.env.step(action)\n        \n        custom_reward = reward\n        \n        hull_angle = obs[2]\n        balance_bonus = 0.3 * (1.0 - abs(hull_angle))\n        \n        progress = obs[0]\n        progress_bonus = 0.1 * progress\n        \n        energy_penalty = 0.00035 * np.sum(np.square(action))\n        \n        custom_reward = custom_reward + balance_bonus + progress_bonus - energy_penalty\n        \n        return obs, custom_reward, terminated, truncated, info\n\ndef make_curriculum_env(render_mode=None, seed=0, difficulty=0):\n    def _init():\n        if difficulty == 0:\n            env = gym.make(\"BipedalWalker-v3\", render_mode=render_mode)\n        else:\n            env = gym.make(\"BipedalWalkerHardcore-v3\", render_mode=render_mode)\n            \n        env.reset(seed=seed)\n        env = CustomRewardWrapper(env)\n        \n        return env\n    return _init\n\ndef make_vec_envs(n_envs=1, seed=0, render_mode=None, difficulty=0):\n    env_fns = [make_curriculum_env(render_mode=render_mode, seed=seed + i, difficulty=difficulty) for i in range(n_envs)]\n    \n    envs = DummyVecEnv(env_fns)\n    envs = VecMonitor(envs)\n    envs = VecNormalize(envs, norm_obs=True, norm_reward=True, clip_obs=10.0, clip_reward=10.0, gamma=0.99)\n    \n    return envs\n\nclass SaveVecNormalizeCallback(BaseCallback):\n    def __init__(self, save_path, verbose=0):\n        super(SaveVecNormalizeCallback, self).__init__(verbose)\n        self.save_path = save_path\n\n    def _on_step(self):\n        if self.n_calls % 100000 == 0:\n            self.model.get_env().save(str(self.save_path))\n        return True\n\ndef linear_schedule(initial_value):\n    def func(progress):\n        return initial_value * (1 - progress)\n    return func\n\ndef train_ppo_bipedal(\n    total_timesteps=20_000_000,\n    n_envs=16,\n    save_dir=\"outputs_improved\",\n    render_video=True\n):\n    out_dir = Path(save_dir) / \"bipedalwalker_ppo_advanced\"\n    ensure_dir(out_dir)\n    ensure_dir(out_dir / \"checkpoints\")\n    ensure_dir(out_dir / \"best_model\")\n    ensure_dir(out_dir / \"video\")\n    ensure_dir(out_dir / \"logs\")\n\n    train_env = make_vec_envs(n_envs=n_envs, seed=0, difficulty=0)\n    eval_env = make_vec_envs(n_envs=1, seed=42, difficulty=1)\n    eval_env.norm_reward = False\n\n    policy_kwargs = dict(\n        net_arch=dict(pi=[512, 256, 128], vf=[512, 256, 128]),\n        activation_fn=nn.ReLU,\n        log_std_init=-0.5,\n        ortho_init=True,\n        share_features_extractor=False\n    )\n\n    model = PPO(\n        \"MlpPolicy\",\n        train_env,\n        verbose=2,\n        n_steps=2048,\n        batch_size=256,\n        n_epochs=10,\n        gamma=0.99,\n        gae_lambda=0.95,\n        clip_range=0.2,\n        clip_range_vf=0.2,\n        normalize_advantage=True,\n        ent_coef=0.01,\n        vf_coef=0.5,\n        max_grad_norm=0.5,\n        target_kl=0.05,\n        learning_rate=linear_schedule(3e-4),\n        tensorboard_log=str(out_dir / \"logs\"),\n        device=\"auto\",\n        policy_kwargs=policy_kwargs,\n        seed=42\n    )\n\n    eval_callback = EvalCallback(\n        eval_env,\n        best_model_save_path=str(out_dir / \"best_model\"),\n        log_path=str(out_dir / \"eval_logs\"),\n        eval_freq=max(1000000 // n_envs, 1),\n        n_eval_episodes=5,\n        deterministic=True,\n        render=False\n    )\n\n    checkpoint_callback = CheckpointCallback(\n        save_freq=max(1000000 // n_envs, 1),\n        save_path=str(out_dir / \"checkpoints\"),\n        name_prefix=\"ppo_bipedal\"\n    )\n    \n    vec_normalize_callback = SaveVecNormalizeCallback(save_path=str(out_dir / \"vecnormalize.pkl\"))\n\n    callbacks = [eval_callback, checkpoint_callback, vec_normalize_callback]\n\n    model.learn(\n        total_timesteps=total_timesteps // 3,\n        callback=callbacks,\n        tb_log_name=\"ppo_bipedal_easy\"\n    )\n\n    train_env.close()\n    train_env = make_vec_envs(n_envs=n_envs, seed=0, difficulty=1)\n    model.set_env(train_env)\n    \n    model.learn(\n        total_timesteps=total_timesteps // 3 * 2,\n        callback=callbacks,\n        tb_log_name=\"ppo_bipedal_hardcore\"\n    )\n\n    model.save(str(out_dir / \"final_model\"))\n    train_env.save(str(out_dir / \"vecnormalize.pkl\"))\n\n    train_env.close()\n    eval_env.close()\n\n    if render_video:\n        env = gym.make(\"BipedalWalkerHardcore-v3\", render_mode=\"rgb_array\")\n        env = RecordVideo(env, str(out_dir / \"video\"), \n                         episode_trigger=lambda x: True,\n                         name_prefix=\"bipedalwalker\")\n        \n        best_model_path = out_dir / \"best_model\" / \"best_model.zip\"\n        model = PPO.load(str(best_model_path), device=\"auto\")\n        \n        vec_normalize = VecNormalize.load(str(out_dir / \"vecnormalize.pkl\"), \n                                         DummyVecEnv([lambda: gym.make(\"BipedalWalkerHardcore-v3\")]))\n        vec_normalize.training = False\n        vec_normalize.norm_reward = False\n\n        obs, _ = env.reset(seed=42)\n        done = False\n        total_reward = 0\n        episode_count = 0\n        \n        while episode_count < 3:\n            normalized_obs = vec_normalize.normalize_obs(obs.reshape(1, -1)).flatten()\n            action, _ = model.predict(normalized_obs, deterministic=True)\n            obs, reward, terminated, truncated, info = env.step(action)\n            done = terminated or truncated\n            total_reward += reward\n            \n            if done:\n                print(f\"Episode {episode_count + 1} finished with reward: {total_reward}\")\n                obs, _ = env.reset()\n                total_reward = 0\n                episode_count += 1\n\n        env.close()\n        print(f\"Video saved in: {out_dir / 'video'}\")\n\nif __name__ == \"__main__\":\n    train_ppo_bipedal(\n        total_timesteps=30_000_000,\n        n_envs=16,\n        render_video=True\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}