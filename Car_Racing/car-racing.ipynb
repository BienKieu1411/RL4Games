{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!apt-get install -y swig\n!pip install box2d-py gymnasium[box2d]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install stable-baselines3\n!pip install gymnasium\n!pip install imageio\n!pip install pygame","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport warnings\nimport gymnasium as gym\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor, VecFrameStack, VecNormalize, VecTransposeImage\nfrom stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, StopTrainingOnRewardThreshold, BaseCallback\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\nfrom gymnasium.wrappers import RecordVideo, TransformReward, ResizeObservation, GrayScaleObservation\n\nos.environ[\"XDG_RUNTIME_DIR\"] = \"/tmp\"\nwarnings.filterwarnings(\"ignore\")\n\ndef ensure_dir(path):\n    Path(path).mkdir(parents=True, exist_ok=True)\n\ndef make_carracing_env(seed=0, render_mode=None):\n    def _init():\n        env = gym.make(\"CarRacing-v2\", render_mode=render_mode, continuous=False)\n        env = ResizeObservation(env, 84)\n        env = GrayScaleObservation(env, keep_dim=True)  \n        env = TransformReward(env, lambda r: r * 0.1)  \n        env.reset(seed=seed)\n        return env\n    return _init\n\nclass ImprovedCNN(BaseFeaturesExtractor):\n    def __init__(self, observation_space, features_dim=512):\n        super().__init__(observation_space, features_dim)\n        n_input_channels = observation_space.shape[0]\n        self.cnn = nn.Sequential(\n            nn.Conv2d(n_input_channels, 32, kernel_size=5, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=1),\n            nn.ReLU(),\n            nn.Flatten()\n        )\n        \n        with torch.no_grad():\n            sample_input = torch.as_tensor(observation_space.sample()[None]).float()\n            n_flatten = self.cnn(sample_input).shape[1]\n            \n        self.linear = nn.Sequential(\n            nn.Linear(n_flatten, features_dim),\n            nn.ReLU(),\n            nn.Linear(features_dim, features_dim),\n            nn.ReLU()\n        )\n\n    def forward(self, observations):\n        if isinstance(observations, np.ndarray):\n            observations = torch.as_tensor(observations).float()\n        return self.linear(self.cnn(observations))\n\nclass SaveOnBestRewardCallback(BaseCallback):\n    def __init__(self, check_freq: int, save_path: str, verbose=1):\n        super(SaveOnBestRewardCallback, self).__init__(verbose)\n        self.check_freq = check_freq\n        self.save_path = save_path\n        self.best_mean_reward = -np.inf\n\n    def _on_step(self) -> bool:\n        if self.n_calls % self.check_freq == 0:\n            rewards = [ep_info[\"r\"] for ep_info in self.model.ep_info_buffer]\n            if len(rewards) > 0:\n                mean_reward = np.mean(rewards)\n                if mean_reward > self.best_mean_reward:\n                    self.best_mean_reward = mean_reward\n                    if self.verbose > 0:\n                        print(f\"New best mean reward: {mean_reward:.2f} - saving model to {self.save_path}\")\n                    self.model.save(self.save_path)\n        return True\n\ndef train_ppo_carracing(\n    total_timesteps=2000000,\n    n_envs=16,\n    reward_threshold=90,  \n    save_dir=\"outputs\",\n    render_video=True,\n    n_stack=4\n):\n    out_dir = Path(save_dir) / \"carracing_ppo\"\n    ensure_dir(out_dir)\n    (out_dir / \"checkpoints\").mkdir(exist_ok=True)\n    (out_dir / \"best_model\").mkdir(exist_ok=True)\n    (out_dir / \"video\").mkdir(exist_ok=True)\n    log_dir = out_dir / \"logs\"\n\n    train_env = DummyVecEnv([make_carracing_env(seed=i) for i in range(n_envs)])\n    train_env = VecMonitor(train_env)\n    train_env = VecTransposeImage(train_env)  \n    train_env = VecFrameStack(train_env, n_stack=n_stack)\n    train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n\n    eval_env = DummyVecEnv([make_carracing_env(seed=42)])\n    eval_env = VecMonitor(eval_env)\n    eval_env = VecTransposeImage(eval_env)\n    eval_env = VecFrameStack(eval_env, n_stack=n_stack)\n    eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True)\n\n    checkpoint_callback = CheckpointCallback(\n        save_freq=max(200000 // n_envs, 1),\n        save_path=str(out_dir / \"checkpoints\"),\n        name_prefix=\"ppo_carracing\"\n    )\n    \n    stop_callback = StopTrainingOnRewardThreshold(\n        reward_threshold=reward_threshold,  \n        verbose=1\n    )\n    \n    eval_callback = EvalCallback(\n        eval_env,\n        callback_on_new_best=stop_callback,\n        best_model_save_path=str(out_dir / \"best_model\"),\n        log_path=str(out_dir / \"eval_logs\"),\n        eval_freq=max(200000 // n_envs, 1),\n        n_eval_episodes=3,\n        deterministic=True,\n        render=False\n    )\n    \n    save_callback = SaveOnBestRewardCallback(\n        check_freq=1000,\n        save_path=str(out_dir / \"best_model\" / \"best_model\")\n    )\n\n    policy_kwargs = dict(\n        features_extractor_class=ImprovedCNN,\n        features_extractor_kwargs=dict(features_dim=512),\n        net_arch=[dict(pi=[256, 256], vf=[256, 256])]\n    )\n\n    model = PPO(\n        \"CnnPolicy\",\n        train_env,\n        verbose=1,\n        tensorboard_log=str(log_dir),\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n        n_steps=1024, \n        batch_size=256,\n        n_epochs=10,  \n        learning_rate=2.5e-4,  \n        gamma=0.99,\n        gae_lambda=0.95,\n        clip_range=0.2,\n        ent_coef=0.01,\n        vf_coef=0.5, \n        max_grad_norm=0.5,\n        policy_kwargs=policy_kwargs\n    )\n\n    model.learn(\n        total_timesteps=total_timesteps, \n        callback=[eval_callback, checkpoint_callback, save_callback]\n    )\n    \n    model.save(str(out_dir / \"final_model\"))\n    VecNormalize.save(train_env, str(out_dir / \"vecnormalize.pkl\"))\n\n    if render_video:\n        video_path = out_dir / \"carracing_video\"\n        ensure_dir(video_path)\n        \n        def make_video_env():\n            env = gym.make(\"CarRacing-v2\", render_mode=\"rgb_array\", continuous=False)\n            env = ResizeObservation(env, 84)\n            env = GrayScaleObservation(env, keep_dim=True)\n            env = TransformReward(env, lambda r: r * 0.1)\n            env = RecordVideo(env, str(video_path), episode_trigger=lambda x: True)\n            env.reset(seed=0) \n            return env\n        \n        env = DummyVecEnv([make_video_env])\n        env = VecMonitor(env)\n        env = VecTransposeImage(env)\n        env = VecFrameStack(env, n_stack=n_stack)\n        \n        env = VecNormalize.load(str(out_dir / \"vecnormalize.pkl\"), env)\n        env.training = False\n        env.norm_reward = False\n        \n        model = PPO.load(str(out_dir / \"best_model\" / \"best_model\"), env=env)\n        \n        obs = env.reset()\n        done = [False]\n        while not all(done):\n            action, _ = model.predict(obs, deterministic=True)\n            obs, rewards, dones, infos = env.step(action)  \n            done = dones\n            \n        env.close()\n        print(f\"Video saved in: {video_path}\")\n\nif __name__ == \"__main__\":\n    train_ppo_carracing(\n        total_timesteps=3000000,  \n        n_envs=16,\n        reward_threshold=90,  \n        render_video=True,\n        n_stack=4\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}